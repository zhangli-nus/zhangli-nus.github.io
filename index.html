<!DOCTYPE html>
<!-- saved from url=(0023)https://zhangli-nus.github.io/ -->
<html lang="en-US"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="google-site-verification" content="-MIZfa2zxR5Mbb499LJJJ-aP5K4EcS6uxH5d6xlW2Mo" />

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Dr Zhang's Homepage</title>
<meta name="generator" content="Jekyll v3.8.5">
<meta property="og:title" content="Zhang Li|张力">
<meta property="og:locale" content="en_sg">
<meta name="description" content="Welcome to Zhang Li’s Personal Homepage">
<meta property="og:description" content="Welcome to Zhang Li’s Personal Homepage">
<link rel="canonical" href="https://zhangli-nus.github.io/">
<meta property="og:url" content="https://zhangli-nus.github.io/">
<meta property="og:site_name" content="zhangli-nus.github.io">
<meta property="Keywords" content="ZhangLi,LiZhang,张力,NUS,ALIBABA">
<script type="application/ld+json">
{"@type":"WebSite","url":"https://zhangli-nus.github.io/","headline":"Zhang Li (张力)","name":"zhangli-nus.github.io","description":"Welcome to Zhang Li’s Personal Homepage","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#157878">
<link rel="stylesheet" href="./zhangli_nus.github.io_files/style.css">
<style type="text/css">
ul li {
    display: inline;
    margin: 0;
    margin-right: 0px;
    padding: 0;
}
</style>
</head>



<body>

<section class="page-header">
<h1 class="project-tagline">Welcome to Dr. Zhang's Personal Homepage</h2>
</section>

<section class="main-content">

<div style="width:auto;margin-left:auto;margin-right:auto">
<table width="100%" border="0">
 <tr>
<td width="50%">
<br/>
<span lang="zh-cn">
            <font size="5" face="georgia"><b>Zhang Li (</b></font><font size="5" face="华文行楷">张力</font><b><font size="5" face="georgia">)</b></font></span>
</div>
<div id="header">
<div id="profile">
<font size="3" face="georgia">
<p>
Staff Research Engineer(高级算法专家)<br>
<i><a href="http://www.alibaba-inc.com" target="_blank">Alibaba Group.</a><br/>
No. 969, Wen Yi West Road, HangZhou, Zhe Jiang Province, China<br/>
</i>
Email: lizhang.nus2010[AT]gmail.com<br/>
</p>
</td>
<!-- <td width="50%"><img src="zhangli_nus.github.io_files/img/personal1.jpg" width="300px"/> -->
</td>
</tr>
</table>
</div>
</div><!--end of header-->


<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="research interest"> <font size="5" face="georgia">Short Bio</font></h2>
<p> <font size="3" face="georgia">
I obtained my Ph.D degree, major in machine learning and related application in computer vision and natural language processing, from School of Computing, National University of Singapore (NUS), where I was supervised by<a href="https://tsim49.wixsite.com/terencesim"> Prof Terence Sim</a>. Currently I am leading a R&D team on machine learning in Alibaba, working on image and text understanding on user content、POI and products. Before that, I was leading a research group in Singapore-based Mozat Pte. Ltd.  for two years.  <br>

<!-- My works on cognition understanding on POI and commodity have been widely adopted on many billion-level applications including Alipay and Taobao. For example, when you search any shops on Alipay search or any products on Taobao search, our algorithm can assess the content quality (e.g. poi status checking, image quality, text quality) and recommend the relevant poi based on your personal requirement(e.g. intention recognition such as brand and category classification).<br> -->

<!-- Up to date, I was granted six patents with fourteen publications shown in <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">my google profile</a>. I'm severing as the local organizer in Alibaba for <a href="http://www.ccks2019.cn/en/?page_id=24">CCKS</a> and an adjunct supervisor in ShenZhen university. I am co-editing a book in Chinese, "Knowledge Graph: from theory to practice".  -->
<br>
In general, I'm devoting my career to develop cutting-edge technology to improve people's life on e-commerce.
</font></p></div>



<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="education"> <font size="5" face="georgia">Education</font></h2>
<p> <font size="3" face="georgia">
&bull;Aug 2010~May 2015 - Ph.D, Computer Science, National University of Singapore, Supervised by<a href="https://tsim49.wixsite.com/terencesim"> Prof Terence Sim. </a>
<br>
&bull;Sep 2006~July 2010 - Bachelor, Computer Science, HuaZhong University Science & Technology with <strong>First Rank Honor</strong>. 

</font></p></div>

<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="working-experience"><font size="5" face="georgia">Working Experience</font></h2>
<p> <font size="3" face="georgia">
&bull;Nov 2017~now - Staff Algorithm Engineer(高级算法专家), Alibaba Group. I'm leading a team to use computer vision and language processing on content understanding and search optimization.
 
<br>
&bull;June 2015-Nov 2017 - Head of R&D, Mozat Pte. Ltd. I set up a R&D team to develop image and text techniques based on deep learning to understand fashion. Our product, <a href="http://www.mozat.com/about#tab2">Stylepedia</a>, was once top ranked in South Eastern Asia. 
</font></p></div>

<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="awards"><font size="5" face="georgia">Awards</font></h2>
<p> <font size="3" face="georgia">
&bull;<!-- <font color=red> -->Excellent Staff in Alibaba during 2018~2019.<!--  <a href="zhangli_nus.github.io_files/img/reward.jpg"> -->See photos</a>.
  <br>
&bull;Excellent Performance in Mozat during 2016 & 2017
  <br>
&bull;<!-- <font color=red> -->Best Student Paper Honorable Mention Award in Automatic Face and Gesture Recognition 2013
  <br>
&bull;The Second Rank Award in Action Recognition Competition in ICPR 2012
<!--   <li>Premia Student Traval Grant Award in ICPR2012</li>
  <li>Research Scholarship in 2010~2014</li> -->
  <br>
&bull;Meritorious Winner in MCM Contest in 2009
  <br>
&bull;National Scholarship in 2007~2009
</font></p></div>

<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="projects"><font size="5" face="georgia">Projects</font></h2>
<p> <font size="3" face="georgia">
  <ol>
    <li><STRONG>POI Search Optimization</STRONG>. May 2018~now in Alibaba<br>
    We build a system on cloud platform, Dataworks, to understand standard point of interest (poi) in billion-level for local shop search. This system can solve many well-known poi problems: a) correct inaccurate attributes for existing poi; b) enrich attributes of existing poi; c) supplement new poi. This project was serving the search on Fliggy、Alipay etc. <!--  This project has improved data quality in Fliggy (from 3M to 10M, accuracy from 58% to 98%), merchant settle-in efficiency in Tmall (200+% improvement) and gained over 100 million direct GMV through hotel reservation. We also use this system to serve for Koubei, Alipay and Eleme. This project has been highly recognized by our department and get rewarded.<br> -->
    My role is in two fold: 1) design the architecture of this system; 2) develop algorithmatic components. Two papers are written and two patents are granted so far.<br>
    Algorithm: Word Embedding (Skip-Gram, CBow, Glove, FastText, Elmo), Sentence Embedding, Text Classification, Image Aesthetic Classification
    </li>
    <li><STRONG>POI Recommendation and Address NER for User Info Center(AI+)</STRONG>. Sep 2019~now in Alibaba<br>
    <!-- User Info Center(UIC) consists of more than 4 billions user info with their mailing address and person information. The mission of our team is to help the company understand users' location so that we can recommend shops and commodity for the users more accurate.<\br>
    Existing address filling in Taobao is purely through text without accurate latitude and longitude, while it is very time-consuming and tedious for customers. This is more severe in local life business, since it demands accurate latitude and longitude for better logistic distribution. Therefore, w -->
    We develop an intelligent system that can help user fill address via simple text pasting and conveniently pick poi through poi recommendation. The main challenge is the noisy input from customers and large number of candidate poi. Our system has been served for Taobao, Tmall, Taoxianda and Maochao.<!--  Our average parsing accuracy is 96% and poi ctr can improve 2%. During this project, We have accumulated 1.4 million building information with over 99% accuracy for Alibaba. One paper is under writing and one patent is grante -->d<br>
    My role in this project is to develop algorithm components, such as address parsing and address matching.<br>
    Algorithm: Sequence Labelling(HMM, CRF, BiLSTM-CRF etc), Pretrain Language Model
    </li>
<!--     <li><STRONG>POI Shopping Guide: Automatic Short Video Generation for Baoshilu(AI+)</STRONG>. Oct 2019~now in Alibaba<br>
    Baoshilu is a shopping guide product in Alibaba. Our work is to help Baoshilu provide video shopping guide capability for merchants. In short, we provide a framework to generate short video with very noisy text and images from merchant and user generated contents. Our system mainly contains two components: 1) attractive image extraction and attractive text generation. 2) image-text matching that utilizes ConceptNet to connect image and text. This system is integrated into hotel release system. The overall accept rate for the synthesized video is 70%. See <a href=zhangli_nus.github.io_files/video/hotel.mp4>demos</a> here. A patent is granted. <br>
    Algorithm: Image Aesthetic Evaluation, ConceptNet for Image-Text Matching, Text Genaration
    </li> -->
    <li><STRONG>Fashion E-commerce from Unconstrained Photos and Virtual Try-on</STRONG>. June 2016~Oct 2017 in Mozat<br>
    Download Stylepedia app and see details. We develop fashion parsing service with Django and Nginx. The service packages include hash-tag recognition (OCR), fashion garment detection (SOLO, FasterRCNN, YOLO etc) and image classification (GoogleNet, Residual Network, VGG etc.) and keypoint based try-on synthesis. The mAP can be up to 0.60 for typical fashion garments and the classification accuracy can be 0.95%. <br>
    Algorithm: Image Detection, Image Classification, Moving Least Square Warpping
    </li>
    <!-- <li><STRONG>Special Face Effect Synthesis</STRONG>. June 2015~July 2016 in Mozat<br> -->
    <!-- We develop many face image editing tools for users. To convert face image into <a href=zhangli_nus.github.io_files/project/moment.jpg>realistic style</a>, we integrate a face detector into poisson image editing algorithm to conduct the synthesis. The accept rate is 95%. For <a href=zhangli_nus.github.io_files/project/mango.jpg>mango style</a>, we use landmark detection to exaggerate the most significant face features. The abnormality of face component is computed through a statistic analysis. For <a href="makeup.html">face makeup</a>, we develop a layer separation technique to synthesize the effect in LAB colorspace. Each face image is warped into the target face image based on the detected facial landmark at beginning. The accept rate is about 94%.  -->
    <!-- These tools were applied on Shabik, a leading chatting app in Middle East. -->
    <br>
    </li>
  </ol>

</font></p></div>

<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="projects"><font size="5" face="georgia">Publications</font></h2>
<table width="780" border="0" cellspacing="0" cellpadding="0">
  <tr>
  <!-- <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en"> -->
  <font size="3" face="georgia"><u><strong>A new TMM paper on content understanding coming soon. </u></strong></font>
</u>
</tr>
  <br>


  <!-- paper one -->
  <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>SentPWNet: A Unified Sentence Pair Weighting Network for Task-specific Sentence Embedding</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia"><u><strong>Li Zhang</u></strong>, Han Wang, Lingxiao Li, Wei Zhang</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/sentpwnet.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">Pair-based metric learning has been widely adopted to learn sentence embedding in many NLP tasks. However, it is known that the sentence representation can be biased when the sampled sentence pairs deviate from the true distribution of all sentence pairs. In this paper, instead of one time pair selection and learning on equal weighted pairs, we propose a unified locality weighting and learning framework. Extensive experiments on four public available datasets and a self-collected place search benchmark with 1.4 million places clearly demonstrate that our model consistently outperforms existing sentence embedding methods with comparable efficiency. http://arxiv.org/abs/2005.11347.</font></p></td>
  </table>
  </tr>
  <br>

  <!-- paper two -->
<!--   <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>Multi-Relational Loss with Inter-Class Locality For Deep Metric Learning</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia"><u><strong>Li Zhang</u></strong>, Lingxiao Li, Han Wang</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/multirelational.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">We propose a new loss, multi-relational loss, on metric learning that can achieve state-of-the-art results on image retrieval and sentence embedding. Existing works have investigated that deep metric learning is about pair sampling and instance weighting, while existing strategy either only considers the influence from the same class or just from self similarity. Instead, our work utilize a more comprehensive weighting schema that each pair is not only relied on inter-class neighboring, but also from intra-class. Our loss takes the inner products of various classes into pair weighting for both negative and positive pairs. We demonstrate that such inner products reveal the correlation between intra-class as an additional regularization term from the perspective of Taylor expansion. The experimental results on both image and text datasets achieved the state-of-the-art results. </font></p></td>
  </table>
  </tr>  
  <br> -->

  <!-- paper three -->
  <tr>
  <a href="https://ieeexplore.ieee.org/document/8737513">
  <font size="3" face="georgia"><u><strong>Brush like a Dentist: Accurate Monitoring of
Toothbrushing via Wrist-Worn Gesture Sensing</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia">Chengwen Luo etc, <u><strong>Li Zhang</u></strong>, Albert Y. Zomaya</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/dentist.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">Oral health has significant impact on people's over-all well-being. In this work, we propose Hygiea, an energy-efficient and highly-accurate toothbrushing monitoring system which exploits IMU-based wrist-worn gesture sensing using unmodified toothbrushes. Hygiea incorporates a number of novel signal preprocessing techniques to automatically transform the sensory input during arbitrary toothbrushing activities to the consistent user coordinate system. To distinguish different brushing actions, Hygiea leverages an emerging deep learning model to achieve fine-grained activity recognitions. Moreover, a POMDP model is incorporated for sampling control to balance activity detection and energy efficiency. Extensive real-world experiments show that the Hygiea system achieves a 11.7% accuracy gain compared to the state-of-the-art while maintaining energy-efficiency and zero modification on the toothbrushes. Published in <font color=red>INFOCCOM2019</font>.</font></p></td>
  </table>
  </tr>  
  <br>

  <!-- paper four -->
  <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>Controllable Face Privacy</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia"><u><strong>Li Zhang</u></strong>, Terence Sim</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/privacy.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">We present the novel concept of Controllable Face Privacy. Existing methods that alter face images to conceal identity inadvertently also destroy other facial attributes such as gender, race or age. This all-or-nothing approach is too harsh. Instead, we propose a flexible method that can independently control the amount of identity alteration while keeping unchanged other facial attributes. To achieve this flexibility, we apply a subspace decomposition onto our face encoding scheme, effectively decoupling facial attributes such as gender, race, age, and identity into mutually orthogonal subspaces, which in turn enables independent control of these attributes. Our method is thus useful for nuanced face de-identification, in which only facial identity is altered, but others, such gender, race and age, are retained. Published in FG2015(ORAL).</font></p></td>
  </table>
  </tr>  
  <br>

  <!-- paper five -->
  <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>New Hope to Recognize Identical Twins: Expression, Ear, Talking and Voice</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia"><u><strong>Li Zhang</u></strong>, Jing Li, Dong Guo, Terence Sim</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/twins.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">Identical twins pose an interesting challenge for recognition systems due to their similar appearance. Although various biometrics have been proposed for the problem, existing works are quite limited due to the difficulty of obtaining a twins database. To encourage the methods for twins recognition and make a fair comparison of them by using the same database, we collected an audio-visual twins database at the Sixth Mojiang International Twins Festival held on 1 May 2010,China. Our database contains 39 pairs of twins in total, including Chinese, American and Russian subjects. This database contains several face images, facial motion videos and audio records for each subject. To address twin recogntion, we propose to use <a href="expression.html">face motion</a>, talking pattern, voice, ear and face recognition. <font color=red>Eight</font> papers are published and we won <font color=red>Best Hornorable Mention in FG2013</font>. </font></p></td>
  </table>
  </tr>  
  <br>

  <!-- paper five -->
  <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>Toward Large-population Face Identification in Unconstrained Videos</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia">Luoqi Liu, <u><strong>Li Zhang</u></strong>, Hairong Liu, Shuicheng Yan</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/largescaleface.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">We investigate large-scale face identification in unconstrained videos with 1000 subjects. This problem is very challenging, and until now most studies have only considered the scenarios with a small number of subjects and videos captured in controlled laboratory environments. First, we set up a large-scale video database in an unconstrained environment, Celebrity-1000, with data collected from video-sharing websites. Second, we boost the efficiency of multitask joint sparse representation (MTJSR) algorithm for video-based face identification on Celebrity-1000. Extensive experiments show several orders-of-magnitude speedup with this new optimization method, and also demonstrate the superiorities of the accelerated MTJSR algorithm over several popular baseline algorithms. Published in IEEE Transactions on Circuits and Systems for Video Technology 2014</font>. </font></p></td>
  </table>
  </tr>  
  <br>

</table>



<!-- <div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="books"><font size="5" face="georgia">Books</font></h2>
<font size="3" face="georgia">
&bull; <strong>Knowledge Graph: From Theory to Practice. </strong>Mechanical Press. To Be Appeared. 
&bull;CN Patent/Innovation: 基于图文匹配的智能实体速览系统 <strong>内部编号: 101247162</strong>, 2019<br>
&bull;CN Patent/Innovation: 强化学习在数据一致性预测的应用 <strong>内部编号: 101052124</strong>, 2019<br>
&bull;CN Patent/Innovation: 一套poi知识自动生产方案 <strong>内部编号: 100617702</strong>, 2019<br>
&bull;CN Patent/Innovation: 基于知识图谱的推理计算方案 <strong>内部编号: 100830983</strong>, 2019<br>
&bull;CN Patent/Innovation: 基于路径优化的智能行程方案 <strong>内部编号: 100830652</strong>, 2019<br>
</font></p></div>
 -->
<!-- <div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="awards"><font size="5" face="georgia">Patents</font></h2>
<font size="3" face="georgia">
&bull;Multitask Address Parsing For POI Address Filling In Taobao. No: CXTA5973<br>
&bull;Visual Storytelling System for Short Video Clips Generation.  CN Patent No. 2020100912116<br>
&bull;Hotel Availability Prediction System via Reinforcement Learning. No: 101052124<br>
&bull;An Automatic Point of Interest Production System Through Entity Alignment. No: 100617702<br>
&bull;A Knowledge Graph Based Inference Framework. No: 100830983<br>
&bull;An Automatic Trip Planner Through POI Knowledge Graph. No: 100830652<br> -->
<!-- 
&bull;CN Patent/Innovation: 基于多任务学习的收货信息填写系统<strong>内部编号: 101247162</strong>, 2019<br>
&bull;CN Patent/Innovation: 基于图文匹配的智能实体速览系统 <strong>内部编号: 101247162</strong>, 2019<br>
&bull;CN Patent/Innovation: 强化学习在数据一致性预测的应用 <strong>内部编号: 101052124</strong>, 2019<br>
&bull;CN Patent/Innovation: 一套poi知识自动生产方案 <strong>内部编号: 100617702</strong>, 2019<br>
&bull;CN Patent/Innovation: 基于知识图谱的推理计算方案 <strong>内部编号: 100830983</strong>, 2019<br>
&bull;CN Patent/Innovation: 基于路径优化的智能行程方案 <strong>内部编号: 100830652</strong>, 2019<br> 
-->
</font></p></div>


<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="awards"><font size="5" face="georgia">Activity</font></h2>
<font size="3" face="georgia">
&bull;Member of local organizer of <a href="zhangli_nus.github.io_files/img/personal3.jpg">CCKS</a> and <a href="zhangli_nus.github.io_files/img/personal4.jpg">Yunxi Conference</a> in 2019<br>
&bull;Invited talk at NanJing University, by Prof. Zheng in Oct 2018.<br>
&bull;Invited talk at ShenZhen University and be an adjunct master advisor, by Prof. Luo in Dec 2017<br>
&bull;Invited talk at Premia AGM, Singapore in 2013<br>
&bull;Student member of Pattern Recognition and Machine Intelligence Association (PREMIA) in 2012<br>
&bull;Teaching Assistant of Computer Vision and Pattern Recognition in 2013<br>
&bull;Teaching Assistant of Uncertainty Modeling in AI in 2012
</font></p></div>


<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="awards"><font size="5" face="georgia">Expertise</font></h2>
<font size="3" face="georgia">
&bull;Deep Learning Framework of Caffe in depth, Pytorch<br>
&bull;Python and C/C++<br>
&bull;Git, Basic Shell, Django, Latex<br>
</font></p></div>


<footer class="site-footer">
<span class="site-footer-credits">All rights reserved@Zhang Li.</span>
</footer>
    </section>
</body>
</html>