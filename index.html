<!DOCTYPE html>
<!-- saved from url=(0023)https://zhangli-nus.github.io/ -->
<html lang="en-US"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="google-site-verification" content="-MIZfa2zxR5Mbb499LJJJ-aP5K4EcS6uxH5d6xlW2Mo" />

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Dr Zhang's Homepage</title>
<meta name="generator" content="Jekyll v3.8.5">
<meta property="og:title" content="Zhang Li|张力">
<meta property="og:locale" content="en_sg">
<meta name="description" content="Welcome to Zhang Li’s Personal Homepage">
<meta property="og:description" content="Welcome to Zhang Li’s Personal Homepage">
<link rel="canonical" href="https://zhangli-nus.github.io/">
<meta property="og:url" content="https://zhangli-nus.github.io/">
<meta property="og:site_name" content="zhangli-nus.github.io">
<meta property="Keywords" content="ZhangLi,LiZhang,张力,NUS,ALIBABA">
<script type="application/ld+json">
{"@type":"WebSite","url":"https://zhangli-nus.github.io/","headline":"Zhang Li (张力)","name":"zhangli-nus.github.io","description":"Welcome to Zhang Li’s Personal Homepage","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#157878">
<link rel="stylesheet" href="./zhangli_nus.github.io_files/style.css">
<style type="text/css">
ul li {
    display: inline;
    margin: 0;
    margin-right: 0px;
    padding: 0;
}
</style>
</head>



<body>

<section class="page-header">
<h1 class="project-tagline">Welcome to Dr. Zhang's Personal Homepage</h2>
</section>

<section class="main-content">

<div style="width:auto;margin-left:auto;margin-right:auto">
<table width="100%" border="0">
 <tr>
<td width="50%">
<br/>
<span lang="zh-cn">
            <font size="5" face="georgia"><b>Zhang Li (</b></font><font size="5" face="华文行楷">张力</font><b><font size="5" face="georgia">)</b></font></span>
</div>
<div id="header">
<div id="profile">
<font size="3" face="georgia">
<p>
Staff Research Engineer(高级算法专家)<br>
<i><a href="http://www.alibaba-inc.com" target="_blank">Alibaba Group.</a><br/>
No. 969, Wen Yi West Road, HangZhou, Zhe Jiang Province, China<br/>
</i>
Email: lizhang.nus2010[AT]gmail.com<br/>
</p>
</td>
<!-- <td width="50%"><img src="zhangli_nus.github.io_files/img/personal1.jpg" width="300px"/> -->
</td>
</tr>
</table>
</div>
</div><!--end of header-->


<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="research interest"> <font size="5" face="georgia">Short Bio</font></h2>
<p> <font size="3" face="georgia">
I obtained my Ph.D degree, major in machine learning and related application in computer vision and natural language processing, from School of Computing, National University of Singapore (NUS), where I was supervised by<a href="https://tsim49.wixsite.com/terencesim"> Prof Terence Sim</a>. Currently I am leading a R&D team on machine learning in Alibaba, working on search and recommendataion of content、POI and goods. Before that, I was leading a research group in Mozat Pte. Ltd  for two years. I'm an experienced engineer to develop cutting-edge technology in e-commerce.
</font></p></div>



<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="education"> <font size="5" face="georgia">Education</font></h2>
<p> <font size="3" face="georgia">
&bull;Aug 2010~May 2015 - Ph.D, Computer Science, National University of Singapore, Supervised by<a href="https://tsim49.wixsite.com/terencesim"> Prof Terence Sim. </a>
<br>
&bull;Sep 2006~July 2010 - Bachelor, Computer Science, HuaZhong University Science & Technology with <strong>First Rank Honor</strong>. 

</font></p></div>

<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="working-experience"><font size="5" face="georgia">Working Experience</font></h2>
<p> <font size="3" face="georgia">
&bull;Nov 2017~now - Staff Algorithm Engineer(高级算法专家), Alibaba Group. I'm leading a team to use computer vision and language processing on content understanding and search optimization.
 
<br>
&bull;June 2015-Nov 2017 - Head of R&D, Mozat Pte. Ltd. I set up a R&D team to develop image and text techniques based on deep learning to understand fashion. Our product, <a href="http://www.mozat.com/about#tab2">Stylepedia</a>, was once top ranked in South Eastern Asia. 
</font></p></div>

<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="awards"><font size="5" face="georgia">Awards</font></h2>
<p> <font size="3" face="georgia">
&bull;<!-- <font color=red> -->Excellent Staff in Alibaba during 2018~2019.<!--  <a href="zhangli_nus.github.io_files/img/reward.jpg"> -->See photos</a>.
  <br>
&bull;Excellent Performance in Mozat during 2016 & 2017
  <br>
<!-- &bull;Best Student Paper Honorable Mention Award in Automatic Face and Gesture Recognition 2013
  <br>
&bull;The Second Rank Award in Action Recognition Competition in ICPR 2012

  <br>
&bull;Meritorious Winner in MCM Contest in 2009
  <br>
&bull;National Scholarship in 2007~2009 -->
</font></p></div>

<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="projects"><font size="5" face="georgia">Projects</font></h2>
<p> <font size="3" face="georgia">
  <ol>
  	<li><STRONG>Merchant Growth and Seller Algorithm for Taobao</STRONG>. May 2020~now in Alibaba<br>
    </li>
    <li><STRONG>Entity Understanding for Local Life Search Optimization for Alipay and Fliggy Search</STRONG>. May 2018~now in Alibaba<br>
<!--     Point of Interest (POI) and goods are important elements in local life. We build a data mining system to understand poi in billion-level. Our system is serving the search on Taobao、Alipay、Fliggy etc.  -->
    </li>
    <li><STRONG>Address Parsing and LBS Recommendation for Taobao</STRONG>. Sep 2019~now in Alibaba<br>
    </li>
    <li><STRONG>Multimodel Fashion Understanding from Unconstrained Photos</STRONG>. June 2016~Oct 2017 in Mozat<br>
<!--     Download Stylepedia app and see details. The service packages include hash-tag recognition (OCR), fashion garment detection (SOLO, FasterRCNN, YOLO etc) and image classification (GoogleNet, Residual Network, VGG etc.) and keypoint based try-on synthesis. The mAP can be up to 0.60 for typical fashion garments and the classification accuracy can be 0.95%. <br> -->
    </li>
    <br>
    </li>
  </ol>

</font></p></div>

<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="projects"><font size="5" face="georgia">Publications</font></h2>
<table width="780" border="0" cellspacing="0" cellpadding="0">


  <!-- paper one -->
  <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>Jointly Multi-similarity Loss for Deep Metric Learning</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia"><u><strong>Li Zhang</u></strong>, Shitian Shen, Han Wang, Lingxiao Li</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/sentpwnet.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">Pair-based metric learning has been widely adopted to learn sentence embedding in many NLP tasks. However, it is known that the sentence representation can be biased when the sampled sentence pairs deviate from the true distribution of all sentence pairs. In this paper, instead of one time pair selection and learning on equal weighted pairs, we propose a unified locality weighting and learning framework. Extensive experiments on four public available datasets and a self-collected place search benchmark with 1.4 million places clearly demonstrate that our model consistently outperforms existing sentence embedding methods with comparable efficiency. Published in <font color=red>ICDM2022</font>.</font></p></td>
  </table>
  </tr>
  <br>

<tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>Discover Micro-influencers for Brands via Better Understanding</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia">Shaokun Wang, Tian Gan, Yuan Liu, <u><strong>Li Zhang</u></strong>, Jianlong Wu</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">With the rapid development of the influencer marketing industry in recent years, the cooperation between brands and micro-influencers on marketing has achieved much attention. As a key sub-task of influencer marketing, micro-influencer recommendation is gaining momentum. However, in influencer marketing campaigns, it is not enough to only consider marketing effectiveness. Towards this end, we propose a concept-based micro-influencer ranking framework, to address the problems of marketing effectiveness and self-development needs for the task of micro-influencer recommendation. Marketing effectiveness is improved by concept-based social media account representation and a micro-influencer ranking function. We conduct social media account representation from the perspective of historical activities and marketing direction. And two adaptive learned metrics, endorsement effect score and micro-influencer influence score, are defined to learn the micro-influencer ranking function. To meet self-development needs, we design a bi-directional concept attention mechanism to focus on brands and micro-influencers marketing direction over social media concepts. Interpretable concept-based parameters are utilized to help brands and micro-influencers make marketing decisions. Extensive experiments conducted on a real-world dataset demonstrate the advantage of our proposed method compared with the state-of-the-art methods. Published in <font color=red>IEEE Transactions on Multimedia 2021</font></p></td>
  </table>
  </tr>
  <br>


  <!-- paper three -->
  <tr>
  <a href="https://ieeexplore.ieee.org/document/8737513">
  <font size="3" face="georgia"><u><strong>Brush like a Dentist: Accurate Monitoring of
Toothbrushing via Wrist-Worn Gesture Sensing</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia">Chengwen Luo etc, <u><strong>Li Zhang</u></strong>, Albert Y. Zomaya</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/dentist.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">Oral health has significant impact on people's over-all well-being. In this work, we propose Hygiea, an energy-efficient and highly-accurate toothbrushing monitoring system which exploits IMU-based wrist-worn gesture sensing using unmodified toothbrushes. Hygiea incorporates a number of novel signal preprocessing techniques to automatically transform the sensory input during arbitrary toothbrushing activities to the consistent user coordinate system. To distinguish different brushing actions, Hygiea leverages an emerging deep learning model to achieve fine-grained activity recognitions. Moreover, a POMDP model is incorporated for sampling control to balance activity detection and energy efficiency. Extensive real-world experiments show that the Hygiea system achieves a 11.7% accuracy gain compared to the state-of-the-art while maintaining energy-efficiency and zero modification on the toothbrushes. Published in <font color=red>INFOCCOM2019</font>.</font></p></td>
  </table>
  </tr>  
  <br>

  <!-- paper four -->
  <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>Controllable Face Privacy</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia"><u><strong>Li Zhang</u></strong>, Terence Sim</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <td width="300"><img src="zhangli_nus.github.io_files/project/privacy.jpg" width="300" border=1 /></td>
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">We present the novel concept of Controllable Face Privacy. Existing methods that alter face images to conceal identity inadvertently also destroy other facial attributes such as gender, race or age. This all-or-nothing approach is too harsh. Instead, we propose a flexible method that can independently control the amount of identity alteration while keeping unchanged other facial attributes. To achieve this flexibility, we apply a subspace decomposition onto our face encoding scheme, effectively decoupling facial attributes such as gender, race, age, and identity into mutually orthogonal subspaces, which in turn enables independent control of these attributes. Our method is thus useful for nuanced face de-identification, in which only facial identity is altered, but others, such gender, race and age, are retained. Published in FG2015(ORAL).</font></p></td>
  </table>
  </tr>  
  <br>

  <!-- paper five -->
  <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>New Hope to Recognize Identical Twins: Expression, Ear, Talking and Voice</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia"><u><strong>Li Zhang</u></strong>, Jing Li, Dong Guo, Terence Sim</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
 <!--  <td width="300"><img src="zhangli_nus.github.io_files/project/twins.jpg" width="300" border=1 /></td> -->
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">Identical twins pose an interesting challenge for recognition systems due to their similar appearance. Although various biometrics have been proposed for the problem, existing works are quite limited due to the difficulty of obtaining a twins database. To encourage the methods for twins recognition and make a fair comparison of them by using the same database, we collected an audio-visual twins database at the Sixth Mojiang International Twins Festival held on 1 May 2010,China. Our database contains 39 pairs of twins in total, including Chinese, American and Russian subjects. This database contains several face images, facial motion videos and audio records for each subject. To address twin recogntion, we propose to use <a href="expression.html">face motion</a>, talking pattern, voice, ear and face recognition. <font color=red>Eight</font> papers are published and we won <font color=red>Best Hornorable Mention in FG2013</font>. </font></p></td>
  </table>
  </tr>  
  <br>

  <!-- paper five -->
  <tr>
  <a href="https://scholar.google.com/citations?user=0yMyTyQAAAAJ&hl=en">
  <font size="3" face="georgia"><u><strong>Toward Large-population Face Identification in Unconstrained Videos</u></strong></font>
  </a>
  </tr>
  <br>
  <tr>
  <font size="3" face="georgia">Luoqi Liu, <u><strong>Li Zhang</u></strong>, Hairong Liu, Shuicheng Yan</font>
  </tr>
  <tr>
  <table width="780" border="0" cellspacing="0" cellpadding="0">
  <!-- <td width="300"><img src="zhangli_nus.github.io_files/project/largescaleface.jpg" width="300" border=1 /></td> -->
  <td width="480" height="200" valign="top"><p align="justify" class="myintro"><font size="3" face="georgia">We investigate large-scale face identification in unconstrained videos with 1000 subjects. This problem is very challenging, and until now most studies have only considered the scenarios with a small number of subjects and videos captured in controlled laboratory environments. First, we set up a large-scale video database in an unconstrained environment, Celebrity-1000, with data collected from video-sharing websites. Second, we boost the efficiency of multitask joint sparse representation (MTJSR) algorithm for video-based face identification on Celebrity-1000. Extensive experiments show several orders-of-magnitude speedup with this new optimization method, and also demonstrate the superiorities of the accelerated MTJSR algorithm over several popular baseline algorithms. Published in IEEE Transactions on Circuits and Systems for Video Technology 2014</font>. </font></p></td>
  </table>
  </tr>  
  <br>

</table>
</font></p></div>


<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="awards"><font size="5" face="georgia">Activity</font></h2>
<font size="3" face="georgia">
&bull;Member of local organizer of <a href="zhangli_nus.github.io_files/img/personal3.jpg">CCKS</a> and <a href="zhangli_nus.github.io_files/img/personal4.jpg">Yunxi Conference</a> in 2019<br>
<!-- &bull;Invited talk at NanJing University, by Prof. Zheng in Oct 2018.<br> -->
&bull;Invited talk at ShenZhen University and be an adjunct master advisor, by Prof. Luo in Dec 2017<br>
<!-- &bull;Invited talk at Premia AGM, Singapore in 2013<br>
&bull;Student member of Pattern Recognition and Machine Intelligence Association (PREMIA) in 2012<br>
&bull;Teaching Assistant of Computer Vision and Pattern Recognition in 2013<br>
&bull;Teaching Assistant of Uncertainty Modeling in AI in 2012 -->
</font></p></div>


<div id="about" style="width:auto;margin-left:auto;margin-right:auto">
<h2 id="awards"><font size="5" face="georgia">Expertise</font></h2>
<font size="3" face="georgia">
&bull;Deep Learning Framework of Caffe in depth, Pytorch<br>
&bull;Python and C/C++<br>
&bull;Git, Basic Shell, Django, Latex<br>
</font></p></div>


<footer class="site-footer">
<span class="site-footer-credits">All rights reserved@Zhang Li.</span>
</footer>
    </section>
</body>
</html>